from crewai import Agent, Task, Crew, Process
from crewai.project import agent, task, crew, CrewBase
from crewai_tools import FileReadTool
from pydantic import BaseModel

import json
class Result(BaseModel):
    expected_coverage: int
    feedback: str
    pass_fail: str  

from crewai import LLM
llm_reasoning = LLM(model='gpt-4o-mini', temperature=0)

@CrewBase
class StaticTester:

    @agent
    def static_logic_tester_agent(self) -> Agent:
        return Agent(
            role="Jest Static Logic Analyzer",
            goal="Analyze a single Jest test file and its source code to identify logical issues without execution",
            backstory="""I am a deep reasoning expert specialized in static analysis of Jest test suites. With extensive knowledge of JavaScript, Jest's mocking system, and software testing principles, I can identify logical flaws in test cases by carefully analyzing the code flow, mock implementations, and test assertions without needing to run the tests.""",
            llm=llm_reasoning,
            tools=[FileReadTool('/content/rmjt_tests/code.js'),FileReadTool('/content/rmjt_tests/code.test.js')]
        )

    @task
    def static_logic_analysis_task(self) -> Task:
        return Task(
            description="""
            Perform a comprehensive static analysis of the single Jest test file and its relationship to the source code.
            
            The test file and source code have been generated by a previous crew and are available in the working directory.
            
            IMPORTANT: Assume that all mocks in the test are correctly implemented. Focus on analyzing the test logic itself.
            
            Without executing the test, analyze:
            
            1. Each test case's logical flow from setup through execution to assertions
            2. Verify test configurations properly use mocks and assertions match expected outcomes
            3. Identify contradictions or impossible conditions in the test logic
            4. Find potential coverage gaps and untested edge cases
            5. For each issue found:
               - Explain the logical problem step by step
               - Show why it would fail
               - Provide specific code changes to fix the issue
            
            YOUR OUTPUT MUST BE STRUCTURED AS A PYDANTIC MODEL WITH THE FOLLOWING FIELDS:
            - expected_coverage: An integer percentage (0-100) indicating how much of the source code functionality is covered by the tests
            - feedback: A detailed string containing all identified issues and SPECIFIC CODE CHANGES to fix each issue
            - pass_fail: Either "PASS" if the tests would execute successfully or "FAIL" if issues were found
            
            The feedback field should include actual code snippets showing both the problematic code and the corrected version.
            """,
            expected_output="""
            A JSON object strictly conforming to the Result pydantic model:
            
            {
                "expected_coverage": 75,  # Example percentage between 0-100
                "feedback": "Issue 1: [Description of issue]\\n\\nCurrent code:\\n```javascript\\n// Problematic code\\n```\\n\\nRecommended fix:\\n```javascript\\n// Fixed code\\n```\\n\\nIssue 2: [Description]...",
                "pass_fail": "FAIL"  # Either "PASS" or "FAIL"
            }
            
            The feedback must include SPECIFIC CODE CHANGES for each issue, showing both the original problematic code and your recommended fixed code.
            """,
            agent=self.static_logic_tester_agent(),
            output_pydantic=Result
        )

    @crew
    def crew(self) -> Crew:
        return Crew(
            agents=[self.static_logic_tester_agent()],
            tasks=[self.static_logic_analysis_task()],
            process=Process.sequential,
            verbose=True
        )
